---
title: Coursera Machine Learning Logistic Regression and Regularization
author: ''
date: '2020-01-01'
slug: coursera-machine-learning-logistic-regression
output: 
  blogdown::html_page:
    toc: true
categories:
  - mooc
tags: []
---

# Classification Problems

> In all of these problems the variable that we're trying to predict is a variable $y$ that we can think of as taking on two values either zero or one, either spam or not spam, fraudulent or not fraudulent, related malignant or benign.

In these type of problems, the variable we are trying to predict take only two values: $Y \in {0, 1}$. Thus, we want to use our knowledge of the training data, $(X, Y)$. But to predict what? One logical way would be to predict the conditional probability $P(Y| X)$.

## Linear Regression?

We could use a linear probability model, where:

$$ P(Y | X; \beta) = x \cdot \beta $$

Given that we are assuming constant marginal effects, $\frac{\partial P(Y | X)}{\partial x_i} = \beta_i$ , regardless of the starting point, it’s possible to have probability predictions outside the interval ${0, 1}$ for observations with extreme values. Thus, awkward and unreliable for prediction. Notice that outliers will have a similar effect: they can easily shift the regression line, and thus shift our predictions in an unwaranted fashion.

> So, applying linear regression to a classification problem often isn't a great idea.

Also, if we assume that the observations are i.i.d. Bernoulli's, the regression will have heteroskedasticity. 

## Logistic Regression

A more logical way, then, is to model $P(Y| X; \beta)$ with a function that will be bounded between 0 and 1. That is, we will use a different ML algorithm that will posit a different hypothesis space. In particular, it will search for our function as a sigmoid function:


$$ P(Y| X; \beta) = \frac{1}{1+e^{-x \cdot \beta}} $$

Notice that this function will always output a value between zero and one. 

## Decision Boundaries

Instead of analyzing our predictions, and our instantiated hypothesis as functions, a more helpful way is to see them as decision boundaries: hyperplanes that divide our training space into a collection of regions labelled according to the classification. To see what I mean, let's create the following prediction criteria: 

$$ \hat{Y} = 1 \to P(Y | X) > 0.5 $$

Which in turn becomes:

$$ \frac{1}{1+e^{-x \cdot \beta}} > 0.5$$
> To summarize what we just worked out, we saw that if we decide to predict whether y=1 or y=0 depending on whether the estimated probability is greater than or equal to 0.5, or whether less than 0.5, then that's the same as saying that when we predict y=1 whenever theta transpose x is greater than or equal to 0.

Which itself becomes: 

$$ e^{-x \cdot \beta} < 1  $$

$$ - x \cdot \beta < ln(1) $$

$$ x \cdot \beta > 0 $$

That is, given some $\beta$, our ML algorithm will classify our training space into two sections: the one where $x \cdot \beta > 0$ and the one where $ x \cdot \beta < 0$. That is, we will have a decision boundary given by the hyperplane $\{x | x \cdot \beta = 0 \}$  .Thus, we can represent our logistic regression as creating a linear decision boundary in our training space. Our training thus, will fit this decision boundary to be optimal in terms of our training data and our loss function. 

**Note, however, that we can augment our training space by creating polynomial features. In that augmented space, we will fit a linear decision boundary; however, in the original space, the decision boundary does not need to be linear.**

*The decision boundary is defined by the parameters of our function, not by the training set*


## Cost Function

Thus, we have learned that the fact that we use a cutoff to predict from a real-valued function implies the creation of a decision boundary within our training space. However, how can we choose the parameters that create the best decision boundary possible? First, we must define what it means to predict well. To do so, let's create a loss function. 

We could use the squared error: 

$$ \ell (x, y; f(x)) = (y - f(x))^2  $$
However, if as $f(x)$ we use the sigmoid function, our loss function won't be convex and thus won't have a global minimum that we can reach with the help of gradient descent. 


### Cross Entropy

We want a loss function that satisfies the following properties:

- When $y =1, f(x) = 1$, $\ell (x, y; f(x)) \to 0$.
- When $y =1, f(x) = 0$, $\ell (x, y; f(x)) \to \infty$.
-When $y =0, f(x) = 0$, $\ell (x, y; f(x)) \to 0$
- When $y =0, f(x) = 1$, $\ell (x, y; f(x)) \to \infty$

Thus, our loss function incentivizes $f(x)$ to get as close as possible to the observed value. The following cost function satisfies these properties:

$$ \sum_M y log(p(w, b; x)) + (1 - y) log (1 - p(w, b; x)) $$

Which has the following derivative:

$$ \frac{\partial C}{\partial x_i} = \frac{1}{m}  \sum^M (h(x; \beta) -  y_i) x_i $$

### A maximum likelihood derivation

Positing that the data comes from a bernoulli:

Thus, the PMF is as follows:

$$ p(y) =  (p)^y (1 - p)^{(1 - y)} $$

If the data comes from an $i.i.d$, the joint probability is simply a multiplication of the respective PMFs of the $x^{i}$. The log of that expression, then, will be the following sum:

$$ \sum_M log((p)^y (1 - p)^{(1 - y))} $$
Applying simple logarithm rules:

$$ \sum_M y log(p) + (1 - y) log (1 - p)  $$

Then, as we switch the function to treat $y, x$ as given, and $w, b$ as parameters, we arrive at:

$$ \sum_M y log(p(w, b; x)) + (1 - y) log (1 - p(w, b; x)) $$

Then, in the maximum likelihood paradigm, we would maximize this quantity. However, we can just put a negative sign in front of it and minimize it. We would then, model $p$ with a logistic function depending on the parameters $w, b$. And voila! we arrive at the aforementioned cost function.  

### Vectorised implementation

Vectorised implementation of sigmoid: simply do the linear combination in one step, $X\beta$, and then apply the sigmoid to each component. 

Vectorised cost function:

$$ C(X, Y, h(X)) = \frac{1}{m} \cdot (-y'log(h(X; \beta)) + (1 - y)'(log(1 - h(X; \beta)))) $$

A vectorised derivative, thus:

$$ X'(h(X; \beta) - Y) $$
## Advanced optimization

"Conjugate gradient", "BFGS", and "L-BFGS" are more sophisticated, faster ways to optimize θ that can be used instead of gradient descent.

## Multi Classification Problem

What if $y \in \{1, 2, 3, .... k\} $. How can we use logistic regression to predict $y$? Use one vs. all method. We create $k$ logistic regressions with $k$ different $y's$: one for each unique value that $y$ can take. For each unique value, all the other classes but the one at hand are lumped together into a new category: thus, $y_i \in \{0, 1 \}$. Then, for each observation, we have a vector of estimates $P(y_i = 1 | X), ...., P(y_i = k| X) $. We simply choose, for each observation, the one category for which we predict the greatest probability. 

## Regularization

There's a function that generalizes well with both the training set and the unseen data. With a ML algorithm, we would like to approximate to this function. However, there's no guarantee any ML algorithm will be able to do it. 

For example, this goal function may not be contained in the hypothesis space that the ML algorithm is considering. For example, the function may be not linear in the parameters and we are using linear regression. **Then, linear regression would suffer from bias**

> as high bias. Both of these roughly mean that it's just not even fitting the training data very well. The term is kind of a historical or technical one, but the idea is that if a fitting a straight line to the data, then, it's as if the algorithm has a very strong preconception, or a very strong bias that housing prices are going to vary linearly with their size and despite the data to the contrary. Despite the evidence of the contrary is preconceptions still are bias, still closes it to fit a straight line and this ends up being a poor fit to the data.

On the other hand, if our hypothesis space is too large, we many not have enough data to distinguish among the possible instantiations of the function: the ones that are truly good, and the ones that just reproduce all the noise present in the data set. That is, small changes in our data may create big changes in our output function: that is, our algorithm will be susceptible to the noise present in our training set. Thus, our ML algorithm is too variable to the training noise to outpuet the best function it can to generalize for our data, as we cannot recognize it. 

> But, the intuition is that, if we're fitting such a high order polynomial, then, the hypothesis can fit, you know, it's almost as if it can fit almost any function and this face of possible hypothesis is just too large, it's too variable. And we don't have enough data to constrain it to give us a good hypothesis so that's called overfitting.

Let's introduce some terminology: the best function is the one that classifies with the true probability of an observation belonging to a class: Bayes' classifier. Our function may deviate from it for any of the two reasons: bias or variance. 

The problem? There’s a trade-off between these two types of error, the bias-
variance trade-off. The bias refers to the mistake of not being close to bayes; the variance that some algorithms won’t yield the same function but will change constantly with the
sample at hand. The largest the space considered, the best we can approximate our best
function to the bayes’, but the more variable will be our function.

### Solutions to overfitting:

1. Reducing the number of features. 

- Manually select which features to keep.
- Use a model selection algorithm (studied later in the course).

That is, our algorithm is overlearning the noise present in the training set. Let's eliminate the variables that contain part of this noise such that it won't learn it. 

1. Regularization.

- Keep all the features, but reduce the magnitude of parameters $\theta$.
- Regularization works well when we have a lot of slightly useful features.

### Cost function

Ideally, we would like to keep a high capacity hypothesis space but we would like to search more carefully among the possible instantiations, such that we will be able to differentiate between good functions and functions that reproduce the noise in the training set. The key is to understand that the noise is propagated through the overuse of some training features. Then, we will instruct our algorithm to only the variables if they reduce our training error but we will penalize for their size. That is, we will have a budget for training variables size: the algorithm will allocate these budget to the most important features, and thus relegating the ones that only contribute through noise. 

Regularizing. High capacity hypothesis function, but be skeptical of using variables. To do so, we can add a **complexity penalizing term to our cost function**: the sum of the squared parameters.

$$ Cost = Cost + \lambda \sum \theta^2$$

Where $\lambda$ controls the degree of complexity penalization. The higher the $\lambda$, the more the cost function will guide our algorithm towards less complex functions. 

The model fitting will be the same as before: initialize parameters, calculate the cost function and its gradient (this step will involve different calculations, given the new parameters) and repeat for the number of epochs in our gradient descent. 

> So using regularization also takes care of any non-invertibility issues of the X transpose X matrix as well.

## Moving towards neural networks

> So whereas linear regression, logistic regression, you know, you can form polynomial terms, but it turns out that there are much more powerful nonlinear quantifiers that can then sort of polynomial regression. And in the next set of videos after this one, I'll start telling you about them.











