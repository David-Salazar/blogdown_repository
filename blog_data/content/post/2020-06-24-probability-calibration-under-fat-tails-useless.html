---
title: 'Probability Calibration under fat-tails: useless'
author: ''
date: '2020-06-24'
slug: probability-calibration-under-fat-tails-useless
categories: []
tags: []
---



<p>Probability calibration refers to a manner of evaluating forecasts: the forecast frequency of an event should correspond to the correct frequency of the event happening in real life. Is this truly the mark of a <em>good analysis?</em> Under fat-tails, Nassim Taleb in <a href="https://www.researchers.one/media/documents/260-m-Technical%20Incerto%20Vol%201.pdf">his book</a> answer with <strong>a categorical response NO!</strong></p>
<div id="probability-calibration-in-the-real-world" class="section level2">
<h2>Probability calibration in the real world</h2>
<p>Probability calibration amounts, in the real world, to a binary payoff: a fixed sum is paid off if the event happens. If one wants to hedge the risk of a fat-tailed variable, the question is then: which lump sum?</p>
<p><img src="/images/mistracking.PNG" /></p>
<p>The answer: there is no possible lump sum that can hedge the exposure to a fat-tailed variable. The reason is the same as to why single-point forecasts are useless:</p>
<blockquote>
<p>There is no typical collapse or disaster, owing to the absence of characteristic scale</p>
</blockquote>
<p>Therefore, given that there is no characteristic scale for fat-tailed variables, one cannot know in advance the size of the collapse nor how much the lump sum of the binary payoff should be.</p>
</div>
<div id="monte-carlo-simulation" class="section level2">
<h2>Monte Carlo simulation</h2>
<p>A quick Monte-Carlo Simulation should do the trick to understand why fat-tailed variables have no characteristic scale. Imagine you are exposed to certain losses. You take a lump-sum insurance. Let’s simulate the possible losses that you may incur if the exposure is a lognormal. For low-values of <span class="math inline">\(\sigma\)</span>, the lognormal behaves as a Gaussian. For higher values, it behaves like a fat-tailed variable.</p>
<div id="log-normal-sigma-0.2" class="section level3">
<h3>Log normal, sigma = 0.2</h3>
<p>With a log normal of sigma = 0.2, a lump-sum of 2 will absolutely cover any of the losses. The reason: <a href="2020-06-10-fisher-tippet-th-a-clt-for-the-sample-maxima.html">the MDA is Gumbel that decays pretty rapidly</a>. Therefore, the sample maxima is effectively bounded at large values from the mean.</p>
<p><img src="/post/2020-06-24-probability-calibration-under-fat-tails-useless_files/figure-html/lognormal-1.gif" /><!-- --></p>
<p>Whereas if we are exposed to a Pareto 80/20, there’s no lump sum that can covers us. The MDA is a <a href="2020-06-10-fisher-tippet-th-a-clt-for-the-sample-maxima.html">Fréchet that decays as a power law</a>:</p>
<p><img src="/post/2020-06-24-probability-calibration-under-fat-tails-useless_files/figure-html/pareto-1.gif" /><!-- --></p>
<p>Given this lack of characteristic scale, there should not be any prize for saying that a there will be a loss larger than <span class="math inline">\(K\)</span>. With fat-tailed variables, almost any larger value is likely. Indeed, fat-tailed variables are long-tailed variables and thus share the following property:</p>
<p><span class="math display">\[
\lim_{x \to \infty} \Pr[X&gt;x+t\mid X&gt;x] =1
\]</span></p>
<p>That is, reducing the variability of a fat-tailed random variable to a binary payoff makes no sense. Therefore, probability calibration with fat-tailed variables makes no sense.</p>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<blockquote>
<p>You do not eat forecasts, most business have severly skewed payoffs, so being calibrated in probability is meaningless.</p>
</blockquote>
</div>
