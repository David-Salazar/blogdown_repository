---
title: 'BDA Week 9: Large Sample Theory for the Posterior'
author: ''
date: '2020-07-13'
slug: bda-week-9-large-sample-theory-for-the-posterior
categories: []
tags: []
---



<p>As Richard McElreath says in his fantastic <a href="https://xcelab.net/rm/statistical-rethinking/">Statistics course</a>, Frequentist statistics is more a framework to evaluate estimators than a framework for deriving them. Therefore, we can use frequentist tools to evaluate the posterior. In particular, what happens to the posterior as more and more data arrive from the same sampling distribution?</p>
<p>In this blogpost, I’ll follow chapter 4 of <a href="http://www.stat.columbia.edu/~gelman/book/">Bayesian Data Analysis</a> and the material in week 9 of <a href="https://github.com/avehtari/BDA_course_Aalto">Aki Vehtari’s course</a> to study the <strong>Posterior Distribution</strong> under the framework of Large Sample Theory.</p>
<div id="asymptotic-normality-and-consistency" class="section level2">
<h2>Asymptotic Normality and Consistency</h2>
<p>Suppose then that the true data distribution is <span class="math inline">\(f(y)\)</span>. If the true data distribution is included in the parametric family, then, for some <span class="math inline">\(\theta_0\)</span> we have <span class="math inline">\(f(y) = \pi(y |\theta_0)\)</span>. <em>Asymptotic consistency</em> then, guarantees that as the sample size increases, our posterior distribution will converge to a point mass at the true parameter value <span class="math inline">\(\theta_0\)</span>. Both the <em>median, the mean and the mode</em> of the posterior are consistent estimators for <span class="math inline">\(\theta_0\)</span>.</p>
<p>Indeed, asymptotic normality, in its turn, guarantees that the limiting distribution of the posterior can be approximated with a Gaussian centered at the mode <span class="math inline">\(\widehat \theta\)</span> of the posterior:</p>
<p><span class="math display">\[
\pi(\theta | y) \approx N(\widehat \theta, [\frac{d^2}{d\theta^2} \log \pi(\theta | y)]_{\theta = \widehat \theta}^{-1} ) 
\]</span></p>
<p>Naturally, we can then approximate the posterior by finding the mode by finding the maximum of the posterior and then estimating the curvature at that point. As more and more data arrives, the approximation will be more and more precise.</p>
</div>
<div id="big-data-and-the-normal-approximation" class="section level2">
<h2>Big Data and the Normal Approximation</h2>
<p>If more and more data makes the normal approximation better and better, does that mean that the era of <strong>Big Data will usher a new era</strong> where the posterior will be easily and reliably <em>approximated with a Gaussian</em>?</p>
<p>Not quite: as we have more and more data, <strong>so we have more and more questions that we can possibly ask</strong>. We can then create <em>more complex models with more parameters</em> to try to answer these more complicated questions. In this scenario, as more and more data arrives, the posterior distribution will not converge to the Gaussian approximation in the expanding parameter space that reflects the increasing complexity of our model. The reason? <strong>The curse of dimensionality</strong></p>
<p>The more dimensions we have, ceteris paribus, due to <strong>the concentration of measure</strong>, the mode and the area around the mode will be farther and farther way from the typical set. Thus, the Gaussian approximation that concentrates most of the mass around the mode will be a poor substitute of the typical set and thus a poor approximation for the posterior distribution.</p>
</div>
<div id="normal-approximation-for-the-marginals" class="section level2">
<h2>Normal Approximation for the Marginals</h2>
<p>Nevertheless, even if we cannot approximate the joint posterior distribution with the Gaussian approximation, the normal approximation is not that faulty if we instead focus on the marginal posterior. The reason? Determining the marginal distribution of a component of <span class="math inline">\(\theta\)</span> is equivalent to averaging over all the other components of <span class="math inline">\(\theta\)</span>, and averaging a family of distributions generally brings them closer to normality, by the same logic that underlies the central limit theorem.</p>
<p>This fact explains why we see so many approximately Gaussian marginals in practice and why can sometimes summarize them with a point estimate and a standard error.</p>
</div>
<div id="unbiasedness-and-hierarchical-models" class="section level2">
<h2>Unbiasedness and Hierarchical Models</h2>
<p>Frequentist methods place great emphasis on unbiasedness: <span class="math inline">\(E[\widehat \theta] = \theta_0\)</span>. However, when parameters are related and partial knowledge of some of the parameter is clearly relevant to the estimation of others, the emphasis on unbiasedness is clearly misplaced: it leads to a naive point in the bias-variance trade-off.</p>
<p>Indeed, if we have a familiy of parameters <span class="math inline">\(\theta_j\)</span> that are related, a Bayesian would fit a hierarchical model by positing a common distribution from which these parameters are sampled. This procedure leads to pooling of the information and thus to shrink the individual parameters <span class="math inline">\(\theta_j\)</span> toward each other; thereby biasing the individual estimates <span class="math inline">\(\theta_j\)</span> but reducing their variance.</p>
<p>Thus, this highlights the problem that it is often not possible to estimate several parameters at once in an even approximately unbiased manner: unbiased <span class="math inline">\(\theta_j\)</span> leads to ignoring relevant information about their common distribution, which in turn leads to a biased estimate of the variance of the <span class="math inline">\(\theta_j\)</span>.</p>
</div>
