---
title: Gini Index under Fat-Tails
author: ''
date: '2020-06-26'
slug: gini-index-under-fat-tails
categories: []
tags: []
---



<p>I have recently been exploring Nassim Taleb’s latest <a href="https://www.researchers.one/media/documents/260-m-Technical%20Incerto%20Vol%201.pdf">technical book: Statistical Consequences of Fat Tails</a>. In this blogpost, I’ll follow Taleb’s exposition of the Gini Index under fat-tails in Chapter 13 of his book.</p>
<p>Intuitively, if we use the “empirical distribution” to estimate the Gini Index, under fat-tails, we underestimate the tail of the distribution and thus underestimate the Gini index. This is <a href="https://david-salazar.github.io/2020/06/11/how-to-not-get-fooled-by-the-empirical-distribution/">yet another example</a> of how we <em>fool</em> ourselves when we are using the “empirical” distribution. Instead, Taleb recommends first understanding the tail behavior of the “Gini Index” <a href="https://david-salazar.github.io/2020/06/11/how-to-not-get-fooled-by-the-empirical-distribution/">by estimating the tail index of the distribution with Maxmimum Likelihood</a> and <em>then</em> using the <strong>functional form</strong> of the maximum likelihood estimator for the Gini Index.</p>
<div id="what-is-the-gini-index" class="section level2">
<h2>What is the Gini Index?</h2>
<p>The Gini index is a measure of concentration commonly used in the income and wealth inequalities discussions. The stochastic representation of the Gini <span class="math inline">\(g\)</span> is:</p>
<p><span class="math display">\[
g=\frac{1}{2} \frac{\mathbb{E}\left(\left|X^{\prime}-X^{\prime \prime}\right|\right)}{\mu} \in[0,1]
\]</span></p>
<p>where <span class="math inline">\(X^{\prime}\)</span> and <span class="math inline">\(X^{\prime \prime}\)</span> are i.i.d. copies of a random variable <span class="math inline">\(X\)</span> with c.d.f. <span class="math inline">\(F(x) \in[c, \infty)\)</span> <span class="math inline">\(c&gt;0,\)</span> and with finite mean <span class="math inline">\(\mathbb{E}(X)=\mu .\)</span></p>
<p>Intuitively, then:</p>
<blockquote>
<p>The Gini Index of a random variable X is the mean expected deviation between any two independent realizations of X, scaled twice by the mean</p>
</blockquote>
</div>
<div id="non-parametric-estimator" class="section level2">
<h2>Non-Parametric estimator</h2>
<p>Using the empirical distribution, then, we can estimate the Gini Index <span class="math inline">\(g\)</span>:</p>
<p><span class="math display">\[
G^{N P}\left(X_{n}\right)=\frac{\sum_{1 \leq i&lt;j \leq n}\left|X_{i}-X_{j}\right|}{(n-1) \sum_{i=1}^{n} X_{i}}
\]</span></p>
<p>We will examine the behavior of this estimator both under thin-tailed distributions and under fat-tailed distributions.</p>
<div id="finite-variance-asymptotic-normality-for-the-non-parametric-estimator" class="section level3">
<h3>Finite variance, asymptotic normality for the non parametric estimator</h3>
<p>Under the hypothesis of finite variance for the data-generating process of <span class="math inline">\(X\)</span>, the estimator is asymptotically normal. We can check this condition with Monte-Carlo simulations. I’ll perform <span class="math inline">\(10^4\)</span> Monte-Carlo experiments: in each of them, I’ll generate a 100 samples from a lognormal with underlying standard deviation of 0.2, which behaves like a Gaussian. Then, I’ll calculate the Gini estimate using the non-parametric estimator.</p>
<p>Given this lognormal, the “true” Gini is thus:</p>
<p><span class="math display">\[
G=2 \Phi\left(\frac{\sigma}{\sqrt{2}}\right)-1 = 0.11246
\]</span></p>
<pre class="r"><code>crossing(experiment = 1:10^4,
         sample_size = 1000) %&gt;% 
  mutate(data = map(sample_size, ~ rlnorm(., sdlog = 0.2)),
         gini = map_dbl(data, ~ gini(.))) -&gt; gini_lognormal

gini_lognormal %&gt;% 
  ggplot(aes(gini)) +
  geom_histogram(binwidth = 0.0005, color = &quot;black&quot;, fill = &quot;dodgerblue4&quot;, alpha = 0.7) +
  geom_vline(aes(xintercept = 0.11246), color = &quot;red&quot;, linetype = 2) +
  labs(title = &quot;Non parametric Gini estimator&quot;,
       subtitle = &quot;Under finite variance, non parametric estimator is asymptotically normal&quot;,
       caption = &quot;Data-generating process is lognormal with underlying sd = 0.2, which behaves like a Gaussian&quot;)</code></pre>
<p><img src="/post/2020-06-26-gini-index-under-fat-tails_files/figure-html/lognormal-1.png" width="768" /></p>
</div>
<div id="fat-tails-non-parametric-estimator" class="section level3">
<h3>Fat-tails: Non-parametric estimator</h3>
<p>However, when the data-generating process of <span class="math inline">\(X\)</span> is in <a href="https://david-salazar.github.io/2020/06/10/fisher-tippet-th-a-clt-for-the-sample-maxima/">the MDA of the Fréchet</a> (i.e., it’s a fat-tailed variable), the non-parametric estimator of the Gini Index loses its properties of normality. Indeed, the limiting distribution of the non-parametric index becomes a skewed-to-the-right <span class="math inline">\(\alpha\)</span>-stable law. Thus, the non-parametric estimate underestimates the true Gini Index.</p>
<p>This can be checked with Monte-Carlo simulations. I’ll perform <span class="math inline">\(10^4\)</span> Monte-Carlo experiments: in each of them, I’ll generate a 100 samples from a Pareto with <span class="math inline">\(\alpha = 1.16\)</span>. Then, I’ll calculate the Gini estimate using the non-parametric estimator.</p>
<p>Given this Pareto, the “true” Gini is thus:</p>
<p><span class="math display">\[
g = \dfrac{1}{2\alpha -1} = 0.7575758
\]</span></p>
<pre class="r"><code>rpareto &lt;- function(n) {
    alpha &lt;- 1.16
   (1/runif(n)^(1/alpha)) # inverse transform sampling
}

crossing(experiment = 1:10^4,
         sample_size = 1000) %&gt;% 
  mutate(data = map(sample_size, ~ rpareto(.)),
         gini = map_dbl(data, ~ gini(.))) -&gt; gini_pareto

gini_pareto %&gt;% 
  ggplot(aes(gini)) +
  geom_histogram(binwidth = 0.01, color = &quot;black&quot;, fill = &quot;dodgerblue4&quot;, alpha = 0.7) +
  geom_vline(aes(xintercept = 0.7575758), color = &quot;red&quot;, linetype = 2) +
  annotate(&quot;text&quot;, x = 0.78, y = 400, label = &quot;True gini&quot;, color = &quot;red&quot;, 
           family = theme_get()$text[[&quot;family&quot;]]) +
  labs(title = &quot;Non parametric Gini estimator&quot;,
       subtitle = &quot;Under fat-tails, non parametric estimator is skewed to the right. Thus, downward bias&quot;,
       caption = &quot;Data generating process is Pareto with alpha = 1.16&quot;)</code></pre>
<p><img src="/post/2020-06-26-gini-index-under-fat-tails_files/figure-html/pareto-1.png" width="768" /></p>
<p>Therefore, under fat-tails, <strong>the non-parametric Gini estimator will approach its true value more slowly, and from below.</strong></p>
</div>
</div>
<div id="the-maximum-likelihood-alternative" class="section level2">
<h2>The Maximum Likelihood alternative</h2>
<p>A better alternative when working with fat-tails, it’s to first estimate the tail and then derive your quantity of interest. Indeed, one does not need too much data to derive the properties of the tail. With a Pareto, for example, the Maximum Likelihood estimator for the tail exponent follows an inverse Gamma distribution that rapidly converges to a Gaussian tightly <em>around</em> the true <span class="math inline">\(\alpha\)</span>. Therefore, one can reliably estimate the tail exponent of the Pareto and thus understand the properties of the distribution with relatively few data.</p>
<p>The ML estimator for the tail exponent of a Pareto is thus:</p>
<p><span class="math display">\[
\widehat \alpha = \frac{n}{\sum _i  \ln (x_i) }
\]</span>
Then, we can derive our Maximum Likelihood estimate for the Gini Index:</p>
<p><span class="math display">\[
g = \dfrac{1}{2\widehat \alpha -1}
\]</span></p>
<p>Indeed, Taleb shows that this estimator for the Gini Index is <strong>not just asymptotically normal, but also asymptotically efficient</strong>. We can test for these using our Monte-Carlo simulations. For each of our simulated datasets, we can derive our Maximum Likelihood estimate and then derive our Maximum Likelihood estimate for the Gini Index.</p>
<pre class="r"><code>estimate_alpha_ml &lt;- function(observations) {
  alpha &lt;- length(observations)/sum(log(observations))
  if (alpha &lt; 1) {
    alpha &lt;- 1.0005 
  }
  alpha
}

gini_pareto %&gt;% 
  mutate(alpha_ml = map_dbl(data, ~ estimate_alpha_ml(.)),
         gini_ml = 1/(2*alpha_ml - 1)) -&gt; gini_pareto

gini_pareto %&gt;%
  rename(nonparametric = gini,
         maximum_likelihood = gini_ml) %&gt;% 
  pivot_longer(c(nonparametric, maximum_likelihood), names_to = &quot;estimator&quot;, values_to = &quot;gini&quot;) %&gt;% 
  ggplot(aes(gini, fill = estimator)) +
  geom_histogram(binwidth = 0.01, color = &quot;black&quot;, alpha = 0.7,
                 position = &quot;identity&quot;) +
  geom_vline(aes(xintercept = 0.7575758), color = &quot;red&quot;, linetype = 2) +
  annotate(&quot;text&quot;, x = 0.78, y = 1000, label = &quot;True gini&quot;, color = &quot;red&quot;, 
           family = theme_get()$text[[&quot;family&quot;]]) +
  scale_fill_viridis_d() +
  theme(legend.position = &quot;bottom&quot;) +
  labs(title = &quot;Comparison of Gini estimators: Non-parametric vs Maximum Likelihood&quot;,
       subtitle = &quot;Under fat-tails, unlike the non parametric estimator, Max Likelihood estimate is still asymptotically normal&quot;,
       caption = &quot;Data generating process is a Pareto with alpha = 1.16&quot;)</code></pre>
<p><img src="/post/2020-06-26-gini-index-under-fat-tails_files/figure-html/comparison-1.png" width="960" />
## Conclusion</p>
<p>When the underlying distribution is fat-tailed, which is always in the case of income or wealth, the non-parametric estimator for the Gini index is skewed to the right and thus underestimates the true Gini index. In this case, it is a much statistically sound strategy to first estimate the tail behavior of the distribution with Maximum Likelihood and then estimate the Gini Index with its plug-in estimator.</p>
</div>
