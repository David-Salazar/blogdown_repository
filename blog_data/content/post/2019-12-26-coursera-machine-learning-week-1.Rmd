---
title: 'Coursera Machine Learning: Week 1'
author: ''
date: '2019-12-26'
slug: coursera-machine-learning-week-1
categories:
  - mooc
tags:
  - machine learning
---

# Why?

As part of my preparation for an upcoming interview, I'll be studying from the ground up the basics of machine learning. I'll use Andrew Ng's course as the bcakbone of my study. For each topic, I'll try to delve a bit deeper into the topic. 

# Week 1

## Why Machine Learning?

*Let the machines program themselves*

> But for the most part we just did not know how to write AI programs to do the more interesting things such as web search or photo tagging or email anti-spam. There was a realization that the only way to do these things was to have a machine learn to do it by itself. So, machine learning was developed as a new capability for computers and today it touches many segments of industry and basic science.

## What is Machine Learning

Arthur Samuel's definition:

> He defined machine learning as the field of study that gives computers the ability to learn without being explicitly learned.

Tom Mitchell's definition:

>  He defines machine learning by saying that a well-posed learning problem is defined as follows. He says, a computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.

## Supervised Learning

> The term Supervised Learning refers to the fact that we gave the algorithm a data set in which the, called, "right answers" were given.

## Regression Problems and Classification Problems

> The term classification refers to the fact, that here, we're trying to predict a discrete value output zero or one, malignant or benign.

## Math Setting

In general, we work in two types of spaces: $X$, the learning space, and $Y$ , the output space. Such that a classification problem is reduced to the question of finding a function $f$ such that: $f: X \to Y$. The machine learning algorithm, then, will be one that given some training examples, we derive a function that performs the classification from the learning space into the output space:

$$ML:\{(X_n, Y_n)\} \subset X \times Y \to f$$

Thus, we want our algorithm to be able to find a function given some training examples, but it is remarked from the beginning that the training examples belong to a wider space of all the possible learning space.

However, we just don't want a function that performs such a task; we want a function that performs the task well. Thus, we first need a definition of what is to do well. *Enter the loss function*: given a function that performs the task, for a given training observation and our prediction, we can come up with a measure of how good the function did:

$$ \ell (x, y, f(x)) $$

Then, we want to find the function that performs the best, according to our loss function, across all the learning space: 

$$ \min_f  E_{(X, Y)}[\ell (x, y, f(x))] $$

That is, we want to find the function $f$ that minimizes the expected loss function across the learning space.

## Example of ML algorithm and a loss function

Let's say we work in a supervised regression problem. We then, can define how well our algorithm performs thus:

$$ \ell (x, y, f(x)) = (y - f(x))^2 $$
We can use the linear regression algorithm, that posits that $f(x)$ is linear. Thus, of all the possible linear functions, we then find the linear function $f(x) = x \cdot \beta$ that minimizes the loss function in our training data.

## Which ML Algorithm for which Loss Function?

As we've seen, the problem thus reduces to that of defining a loss function, and, according to our learning algorithm, find the function that best performs the given task. A logical question, then, may be: which learning algorithm is best?

The answer, it depends. The *mo free lunch* theorem states that we cannot stablish a ranking of all the possible learning algorithms. Sometimes, one learning algorithm may perform better than others. In particular, on average over all probability distributions, no classifier can be better than random guessing on the test set!

## Unsupervised Learning

>  In Unsupervised Learning, we're given data that looks different than data that looks like this that doesn't have any labels or that all has the same label or really no labels.

The most common problem in unsupervised leraning is clustering: find the latent structure in the data to group different observations into different clusters of similar looking observations. For example, market segmentation.

> Unsupervised learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don't necessarily know the effect of the variables.

> We can derive this structure by clustering the data based on relationships among the variables in the data.

> With unsupervised learning there is no feedback based on the prediction results.


