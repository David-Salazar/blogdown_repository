---
title: Trees, Ensembles and beyond, XGBoost and LGBM
author: David Salazar
date: '2018-06-10'
slug: trees-ensembles-and-beyond
output: 
  blogdown::html_page:
    toc: true
categories:
  - tutorial
tags:
  - machine learning
---

# Why?

`lightgbm` and `xgboost` appear in every single competition at Kaggle. Thus, these boosting techniques must be able to learn something that cannot be easily learned from intelligent bagging techniques like Random Forests. This is my attempt to understand _why_ and _how_ they can do that.

## Set-up

This will be kind of mathy, but I will try to keep the notation as clear as possible so it can be understood without much hassle. The traditional problem in machine learning, for a given algorithm, is as follows: learn a function, $f(x; \theta)$ such that $\theta$ minimizes the following:

$$ E_{y,x} (L(y, f(x; \theta)) ) $$
Unpacked, it can be explained thus: given a loss function, $L$, we want to find the $\theta$ such that the loss between the observations, $y$, and our predictions, $f(x;\theta)$ is minimized across the values we expect to see of the observations $\{x, y\}$. Note that this last condition precludes *train-set-overfitting*: we want something to generalizes even to not-yet seen observations. 

# Trees

From the freely available [Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) p.305:

> Tree-based methods partition the feature space into a set of rectangles, and then fit a simple model (like a constant) in each one. They are conceptually simple yet powerful.

Thus, a prediction with a tree is a matter of finding on **which partition** our new observation sits and predict **the constant assigned** to that region. For regression problems, the constant is the average of the observations in the regions; for classification problem, the constant is the modal class within the region. 

## Fitting them

They sound, and are quite simple, but the big problem is **how to find the partitions** in the first place. Mathematically, the problem can be defined thus:

For $J$ partitions that we decide to perform, find where in the space to do the partition, $R_j$, and what constant to assign to that partition, $c_j$. We must find, for each partition $j$ the $R_j$, $c_j$ pair that minimize the following: 

$$ \sum_{j}^J \sum_{x_i \in R_j} E_y (L(y, c_j) ) $$

However, this a prohibitely expensive combinatorial optimization problem. To avoid this, we proceed with a **greedy algorithm**: 

For any given node do the following:

1. Loop over each of the features.
2. For each feature, sort the observations and find every possible split using this feature. Compute the **information gain** of doing each of these splits. Keep only the one that gave the best information gain. 

> Information gain is defined as the difference between the loss in the node and the average loss across the nodes created as we do the proposed partition

3. Compare the best information again across features. Finally, use the feature that had the best one. 
4. Go over the next node and repeat.

Thus, note that the algorithm will keep doing this partitions as long as there are no possible partitions in any of the nodes. 

## Interpretation 

Besides the traditional tree visualization, notice that we can rank the importance of each feature according to their predictive power:

> For each node, annotate which feature was used and the resulting information gain from that partition. Do that across all the nodes. Sum for all the features that were used more than once. The features that have more of these values, are the ones that were more important for our tree.

**This way of computing feature importances will be generalized to ensemble trees. As we will do the same procedure for each tree in the ensemble, and then average across trees. However, this procedure is not without its [perils](http://parrt.cs.usfca.edu/doc/rf-importance/index.html).**

# Ensembles

Trees are simple and powerful. Yet, they are limited: they are extremely susceptible to noise: i.e., they have a high variance. From [Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf), p.312:

> The major reason for this instability is the hierarchical nature of the process: the effect of an error in the top split is propagated down to all of the splits below it.

However, **they can be much improved if they are combined together**. 

## Bagging

Bagging consists on the idea of combining **weak learners**, that is, models that do not perform very well, and that the end result is greater than the sum of its parts. Why can we do this? When we have multiple and **uncorrelated weak learner**, they will make mistakes in their individual predictions, yes, but they will commit **different types of mistakes**. If they are **roughly right**, some models will overshoot and some will undershoot: the end result of combining them will be that we will make correct predictions as the **errors will cancel each other**. 

In the statistical learning lingua, what we are trying to achieve is, by virtue of combining **uncorrelated weak learners**, a better position in the **bias-variance trade-off**. To do so, we need to enforce the following conditions: 

1. The individual **weak learners** must have **low correlation between each other** and have **low bias**(i.e., they must able to learn complex data structures). 
1. The combination of the models **cannot increase the bias** that the individual learner has.
1. The combination of the models must **reduce the overall model variance** with respect to the variance of the individual weak learners. 

If the three conditions hold, bagging will lead us into a better position in the **bias-variance trade-off**: we will have the same bias (which is already low, given how powerful indiviual trees can be) but we will have lower variance. Thus, we have a better model. 

To make sure that three conditions mentioned hold, there are two smart methods for bagging: **simple bagging and random forests**. In both, we combine low bias trees and try to decorrelate them by using some clever tricks such that the trees that we construct end up being **uncorrelated weak learners**. Note that the key part here is "**uncorrelated weak learners**". **If they are weak learners, but are highly correlated, combining them won't fix any of the problems the individual learners have**.

Mathematically (without worrying about its derivation, just the interpretation), this can be seen thus:

* The variance of an average of $B$ $i.i.d.$ random variables ($X$ with variance $\sigma^2$) is:

$$ \frac{1}{B}\sigma^2\ $$

* However, if they are identically distributed but not independent, and with positive pairwise correlation $\rho$, the variance of their sum average:

$$ \rho \sigma^2 + \frac{1 - \rho}{B} \sigma^2  $$

Thus, as $B$ increases and $\rho$ decreases, our individual weak learners will become more like the **uncorrelated weak learners** we need for bagging to be effective. Then, the conditions for effective bagging are clear: **we must be able to create a large number of weak learners them (large $B$) and we must be able to create decorrelated versions of them**. It just so happens that, with some ingenuity, trees are the perfect candidate!.  Both **Bootstraping and Random Forests** are ways of achieving this succesful ensemble with trees. 

Once we have multiple **uncorrelated weak learners**, combining them is a simple task. If the problem is a regression one, we average our predictions across trees. If it is a classification one, we use majority voting. 

### Bootstraping

So we know how to construct high variance low bias trees: 

 > Construct them such that they have a lot of depth and thus are able to capture complex structure in the data. This can be accomplished by setting a low value for `min_samples_leaf` in the `scikit-learn` implementation. Although each of the trees will tend to overfit as they have more depth, **bagging will precisely solve this problem** by reducing the overall model variance.

**But how to construct them such that they are uncorrelated between them?**

One way of making sure that the trees are uncorrelated is to make them use slightly **different samples** in their respective training. That is, for each tree that we construct, we are going to pass them over a bootstrap-resampled version (i.e., sampled $rows$ with $n = |trainingdata|$ times with replacement) of the training data. Thus, as each tree is going to work with different data, they are going to learn 
**different partitions** and thus predict different values and behave like **uncorrelated weak learners** we need. 

To understand this, let's make the following experiment. imagine one tree and one bootstrap-resampled version of the training data. For a given row, **what is the probability that the tree will see this row in its data?** 

> The probability of not choosing that row in the first iteration of the bootstrap is: $\frac{n-1}{n}$. The probability of not choosing that row in any of the other iterations is: $(\frac{n-1}{n})^n$ (remember that we are sampling n times from the n rows). Thus, the probability of the tree seeing that row in its data, is: $1- (\frac{n-1}{n})^n$

Using computer power, we can see that this expression converges to 63%:

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(dplyr)
samples <- seq(10, 1000, 10)
probs <- 1 - ((samples-1)/samples)^(samples)
df <- data.frame(cbind(samples, probs))
df %>% 
  ggplot(aes(x = samples, y = probs)) +
    geom_line() +
    hrbrthemes::theme_ipsum_rc() + 
    scale_y_continuous(labels = scales::percent) +
    labs(x = "# of obs training and bootstrap iterations",
         y = "Probability of seeing row in data",
         subtitle = "How likely is that the tree will see this row?",
         title = "Probability with iterations = # training data")
```

Thus, **if we perform a bootstrap with equal number of iterations as there is # of observations in the training data, the tree will see at least that row in its data with 63% of probability**. Not bad right? **That means that there's a 37% chance that it will not see it, will not learn from it, and thus will arrive at a different partition** than the trees that do see that observation. Notice that we are moving on the **bias-variance tradeoff**: if the tree does not see it, it will be biased; however, when averaging with others, this will be corrected **and** these differences will reduce the overall model variance. 

However, we can be even more extreme with this procedure. **We can perform a bootstrap by sampling even less times than there are training observations**. If we do so, the probability that, for a given row, a given tree will not see it in its data is: 

$$1- (\frac{n-1}{n})^k $$

**Remember that now $k < n$**. Evaluating this operation such that $k \in \{n/2, n/3, n/4\}$:

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(purrr)
n_less_k <- function(k) {
  samples <- seq(10, 1000, 10)
  probs <- 1 - ((samples-1)/samples)^(samples/k)
  df <- data.frame(cbind(samples, probs))
  df %>% 
    mutate(bootstrap_iterations = k)
}

label_ <- function(k) {
  paste0("# of iter = n/",k)
}
c(1, 2, 3, 4) %>% 
  map_df(n_less_k) %>% 
  ggplot(aes(x = samples, y = probs)) +
    geom_line() + 
    geom_hline(yintercept = 0.63, linetype = 2, color = "red4") +
    hrbrthemes::theme_ipsum_rc() + 
    scale_y_continuous(labels = scales::percent) +
    scale_x_continuous(breaks = c(0, 500, 1000)) +
    facet_grid(~bootstrap_iterations, labeller = labeller(bootstrap_iterations = label_))+
    labs(x = "# of obs in training",
         y = "Probability of seeing row in data",
         subtitle = "How likely is that the tree will see this row?",
         title = "Probability with iterations < # training data")
```

Notice how the probability of **seeing a given row in a given tree decreases as we reduce the number of iterations in the bootstrap**. This means that more and more trees won't see some observations, won't learn from them and thus will learn different partitions from the trees that do see those observations. Thus, **by reducing the number of iterations in the bootstrap we can reduce the correlation between the trees**. However, notice that this will accomplish an even greater shift in the bias-variance tradeoff: the reduction in the variance will be greater, yes, but so will be the increase in the bias of each individual tree. Also, this technique has the benefit of reducing training time, as each tree takes less time in training due to lower number of observations (and consequently, # of splits) it considers. 

**Note that the only way I know of setting the number of bootstrap iterations lower than the number of observations is by using the `fast.ai` library and the following two functions:**

```{r message=FALSE, warning=FALSE, include=FALSE}
library(reticulate)
reticulate::use_python(python = "/anaconda3/bin/Python", required = TRUE)
```


```{python}
from sklearn.ensemble import forest

def set_rf_samples(n):
    """ Changes Scikit learn's random forests to give each tree a random sample of
    n random rows. from fast.ai: https://github.com/fastai/fastai/blob/master/fastai/structured.py
    """
    forest._generate_sample_indices = (lambda rs, n_samples:
        forest.check_random_state(rs).randint(0, n_samples, n))
        
def reset_rf_samples():
    """ Undoes the changes produced by set_rf_samples.
    """
    forest._generate_sample_indices = (lambda rs, n_samples:
        forest.check_random_state(rs).randint(0, n_samples, n_samples))

```

### Random Forests

Even though passing over different bootstraped-versions to each tree in a forest can effectively "decorrelate" the trees, we can improve on it. Let's think of a situation where **this procedure fails**: no matter what rows each tree sees, the partitions are all the same. Why? Close to all of the **observations share a predominant** feature: one that always will yield the best information gain if split on it, no matter what subset of the data the tree is seeing. Thus, the trees won't give different predictions and bagging won't help much. What can we do? 

Enter **random selection**: every time any of the trees will evaluate which feature to use, we **will tell it to only consider a subset of the features**. Which subset? Well, **every time any of the trees reach any of the nodes, we will draw a random sample of the subsets** and the tree will only be allowed to look within that subset for the best split. 
Now consider the former problem of the predominant feature. What will happen? Each time a node evaluates on which features to split, **the predominant feature may or may not be within the subsets of features the tree is allowed to use in that node**. If it is not, then the tree won't use that feature: even if could have used that feature and improve the information gain from that split. **Why do we choose to sacrifice individual tree performance?** This loss in the information gain at a particular tree is **countered by the overall model benefits of having trees that uncorrelated between them and thus reaping the benefits of bagging**. 

From the [Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) p.598, we can see that, effectively, tree correlation decreases as we are more stringent with the number of features that each node can use:

![](/post/2018-06-05-trees-ensembles-and-beyond_files/esl.png)

### Conclusions for Bagging

Ensembling trees into a bigger model will be a good idea when the trees are enough to understand the complexity of the structure in the data and when we can construct the trees such that the correlation between them is low enough. One way of tackling the first issue is constructing trees with enough depth. **One way of tackling the second issue is by using bagging and random forests.**

Thus, there are **four hyperparameters that you have to set to define in which position of the bias-variance trade-off** you want to be when using the tree ensembles discussed above:

* $B$: The number of trees you want to use to average predictions. In `scikit-learn`: `n_estimators`. 

> This type of ensemble is pretty difficult to get to overfit with this hyperparameter. You will see that as you change this number, predictions won't change that much.

* $k$: How many samples of the training data you want to pass to each individual tree. Use fast.ai's function `set_rf_samples()`.

* The size of the random subset of features that each node can use to determine which is the best split. The less, the more "decorrelated" the trees will be. In `scikit-learn`: `max_features`. 

* The individual tree depth that each tree can go to. This is easily determined by `min_samples_leaf` in `scikit-learn`, which determines the minimum number observations required to be at a leaf node. 

In code this will look like:

```{python eval=FALSE}
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

set_rf_samples(k)
model = RandomForestRegressor(n_estimators=B, min_samples_leaf=3, 
                      max_features=0.5, n_jobs=-1)
```

## Boosting 

Boosting is also an ensemble of weak learners technique, but **a fundamentally different one than Bagging**. The way it works is by **sequentially fitting weak learners models to sequentially modified versions of the data**. The key is in *sequentially modified versions of the data*. With boosting, we want each **model to focus on the structure within the data that the former model got didn't learn effectively**. Thus, each learner will be directed to focus on different structures within the data. 

Putting this more formally. **Sequentially means that we will repeat one process for a given number of iterations. Those are the number of boosting iterations**. For the first iteration we will treat it as any other supervised learning task. That is, we will, fit a tree such that we minimize the following:

$$ E_{y,x} (L(y, T_1(\{R_{j,1}\}, \{c_{j,1}\},x)) ) $$

Were $\{R_{j,1}\}$, $\{c_{j,1}\}$ are the sets of regions and predictions we create by fitting this first tree. 

However, for any further iteration, let's say the second iteration, we will be:

1. trying to find another function,$T_{2}(\{R_{j,2} \}, \{c_{j,2} \},x)$ such that we minimize our iteration's loss function.
2. We are treating the trees we fitted at the other iterations as set in stone.

Thus, our optimization problem for the second boosting iteration is:

$$E_{y,x}(L(y, T_1(\{R_{j,1} \}, \{c_{j,1} \},x) + T_{2}(\{R_{j,2} \}, \{c_{j,2} \},x)))$$

Notice the big change, **within the loss now there are terms set in stone at both sides of the comma:** $y$ and $T_1(\{R_{j,1}\}, \{c_{j,1}\},x)$. Notice **how the complexity of finding the best splits for** $T_{2}(\{R_{j,2} \}, \{c_{j,2} \},x)$ **changed with respect to the first tree**: for any given split we are considering, we must first find what predictions we gave before for the observations at either side of the split. With more boosting iterations and thus more trees, this problem grows even more complex. 

To solve this problem, we will take a detour into **directional derivatives and steppest descent**. Then, we will be able to solve the above problem with a very clever approximation. 

### Directional Derivative

Let's make an experiment for a second and forget the above. Let's think of the problem of only predicting one row, $row$. Let's forget $T_{2}( \{R_{j,2}\}, \{c_{j,2}\},x)$, let's treat $y$ as a parameter and let's treat $T_1(\{R_{j,1}\}, \{c_{j,1}\},x)$ as a variable $z$. And finally, let's forget the expectation. Then, we end up with a function in one variable: 

$$ L(z ; y_{row}) $$

Imagine we have to minimize this function by taking little steps around a starting point. That is, for a starting point, $z_0$, which $addition$ is the one that minimizes: $$  L(z_0 + addition; y_{row}) $$

From a bit of calculus (do not worry about the derivation, just the interpretation), we know that: 

$$ L(z + addition; y_{row}) \approx L(z_0; y_{row}) + \nabla  L(z_0; y_{row}) \cdot addition $$
$\nabla  L(y_{row}, z)$ is the gradient of the function. Remember: the gradient is just a vector, each component tells you, for a given axis, the rate of change of the function in the direction of that axis. By taking the dot product with $addition$, which can also be seen as a one dimensional vector, we have the rate of change of the function in the direction of that vector. **This is called the directional derivative** of the function in the direction of the vector $addition$.  

Thus, **our problem now is to find direction (the vector) on which decreases our function the most**. Because, $L(z_0; y_{row})$ is a constant, our problem is minimizing the dot product $\nabla  L(z_0; y_{row})$. For our problem to make sense (i.e., we cannot say our direction is minus infinte across any axis), we will restrict ourselves to close by vectors. Were we to solve this problem sequentially, that is, **find the $addition^*$ that minimizes our function around some point, move to that point, and then solve the problem again**, we would be performing **the steepest descent method** and end up minimizing our function $  L(z; y_{row}) $

### Gradient Boosting: Back to our problem

Notice that the problem of boosting is equivalent of the problem we just solved: 

1. Start around a given point in a function you want to minimize. **That would the tree we fit in the first boosting iteration**.
2. Add a term to our prediction **such that we move in the direction that minimizes our loss function**. We know in which direction the function minimizes if we use **directional derivatives and Steppest descent**. Thus, we must make that the term we are adding, **the predictions of our second tree**, as close as possible as to this value. **The best way to do so? Direct this second tree to predict these values!**.
3. Repeat (2) for further boosting iterations. Because we are essentially performing **Steppest Descent**, we will end up minimizing our loss function. 

Using math notation:

1. Fit a tree in the first boosted iteration $T_1(\{R_{j,1} \}, \{c_{j,1} \},x)$.
1. Evaluate loss function around the point that is our prediction with the first tree. **We want to move that point in the direction** (which is the same as adding $addition$) such that we **minimize our loss function**. We already know which vector solves this problem!! **The vector**$addition$ **that minimizes**$\nabla L(y_0, T_1(\{R_{j,1} \}, \{c_{j,1} \},x_0)) \cdot addition$. **Now that we know the solution to our problem the question is: can we arrive at this** $addition^*$ **by using a random forest?**. 

> Well the only way we can know is by trying. **Let's repeat the process for each row such that we have an $addition^*$ to predict for each row and then put them all in a vector**. Then, let's fit our second boosted iteration tree, $T_{2}(\{R_{j,2} \}, \{c_{j,2} \},x)$, but **this time trying, instead of trying to predict $y$ again, we want to predict this new vector of** $addition^*$'s. Once we have predicted these, add those to our first tree predictions. The result of this addition is now our current prediction.  

> Note that each tree predicts a different quantity: **a quantity that measures where the biggest improvement, given the past trees' predictions, can be done**. This is the "sequentially fitting weak learners models to sequentially modified versions of the data". 

3. **If we do a good job across boosting iterations repeating (2), we would be updating** $L(y_{row}, T_1(\{R_{j,1} \}, \{c_{j,1} \},x) + T_{2}(\{R_{j,2} \}, \{c_{j,2} \} + ...,x_0))$ by a value close enough to $addition^*$ at each iteration. If we repeat the process across the boosting iterations, **we will be essentially performing Stepping Descent and will be minimizing our loss function**. 



Notice that it wasn't a coincidence that we used the term **addition** when describing **directional derivatives and Steppest descent**. When performing step (2), we are literarlly adding to our predictions up to that point the predicted $addition$'s. When we are in a regression problem, this makes perfect sense: we are adding real-valued terms one after another. However, in classification problems, we cannot continually add to our first prediction, a class (or probability), the real-valued terms that we start predicted from the second iteration onwards. To solve this problem, to all the trees that we use to predict some version of $addition$, before adding their prediction to the first prediction we did, **we update their leaves: we enforce the rules we use in any common classification decision tree, e.g., majority voting** For further details, I recommend you look up [Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) p.360 or the source code of the [`scikit-learn`](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/gradient_boosting.py) function: `GradientBoostingClassifier`.


requirements: define a good transformation of the data such that it focues the next learner on the unseen parts of the data. fit a good model to each part of the data each model is directed to. combine them such that the models that best model each part of the data are given more weight than others. 

Meta parameter: number of boosts; size of the trees; shrinkage; v? ; subsampling?










